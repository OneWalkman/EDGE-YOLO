# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""Activation modules."""

from __future__ import annotations
import torch
import torch.nn as nn
import torch.nn.functional as F

class AGLU(nn.Module):
    """Unified activation function module from https://github.com/kostas1515/AGLU."""

    def __init__(self, device=None, dtype=None) -> None:
        """Initialize the Unified activation function."""
        super().__init__()
        self.act = nn.Softplus(beta=-1.0)
        self.lambd = nn.Parameter(nn.init.uniform_(torch.empty(1, device=device, dtype=dtype)))  # lambda parameter
        self.kappa = nn.Parameter(nn.init.uniform_(torch.empty(1, device=device, dtype=dtype)))  # kappa parameter

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Compute the forward pass of the Unified activation function."""
        lam = torch.clamp(self.lambd, min=0.0001)
        return torch.exp((1 / lam) * self.act((self.kappa * x) - torch.log(lam)))

# ===== æ ¸å¿ƒçš„å‡½æ•°å¼å®ç° =====
@torch.jit.ignore
def _stable_tanh_exp(x: torch.Tensor, cutoff: float = 20.0) -> torch.Tensor:
    """
    è®¡ç®— tanh(exp(x)) çš„æ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼š
    - å¯¹äº x >> 0ï¼Œtanh(exp(x)) â‰ˆ 1ï¼Œå› æ­¤ç›´æ¥è¿”å› 1ï¼Œé¿å… exp æº¢å‡º
    - å¯¹äºå…¶ä½™åŒºåŸŸï¼Œå®‰å…¨åœ°è®¡ç®— tanh(exp(clamp(x, max=cutoff)))
    """
    # å¤§æ­£å€¼è¿‘ä¼¼ï¼štanh(exp(x)) â‰ˆ 1
    large_pos = x > cutoff
    if large_pos.any():
        # å…¶ä½™ä½ç½®æŒ‰æ­£å¸¸å…¬å¼è®¡ç®—ï¼Œä½†æŠŠ x é™åˆ¶åˆ° cutoff ä»¥å†…é¿å… exp æº¢å‡º
        e = torch.exp(torch.clamp(x, max=cutoff).to(torch.float32)).to(x.dtype)
        t = torch.tanh(e)
        t = torch.where(x > cutoff, torch.ones_like(x), t)
    else:
        e = torch.exp(torch.clamp(x, max=cutoff))
        out = torch.tanh(e)
    return out

def telu(x: torch.Tensor, stable: bool = True, cutoff: float = 20.0) -> torch.Tensor:
    """
    TeLU æ¿€æ´»ï¼šy = x * tanh(exp(x))
    å‚è€ƒè®ºæ–‡ç»™å‡ºçš„å®šä¹‰ä¸å¯¼æ•°å½¢å¼ã€‚:contentReference[oaicite:2]{index=2} :contentReference[oaicite:3]{index=3}

    å‚æ•°
    ----
    x : Tensor
    stable : bool
        æ˜¯å¦å¯ç”¨æ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼ˆæ¨èï¼‰ã€‚å¯¹å¤§æ­£æ•°ä½¿ç”¨è¿‘ä¼¼ yâ‰ˆxï¼ˆå›  tanh(exp(x))â‰ˆ1ï¼‰ï¼Œ
        å¯¹æè´Ÿæ•°é‡‡ç”¨å®‰å…¨çš„ exp é™å¹…è®¡ç®—ã€‚
    cutoff : float
        ç¨³å®šè®¡ç®—çš„æ­£å‘æˆªæ–­é˜ˆå€¼ï¼ˆfloat32 ä¸‹ 20 å·²è¶³å¤Ÿè®© tanh(exp(x))â‰ˆ1ï¼‰ã€‚

    è¿”å›
    ----
    y : Tensor
    """
    if stable:
        t = _stable_tanh_exp(x, cutoff)
        return x * t
    else:
        # ç›´æ¥çš„æ•°å­¦å®šä¹‰ï¼ˆå¯èƒ½åœ¨å¤§æ­£æ•°æº¢å‡ºåˆ° inf ä½†ä»å¯ç”¨ï¼‰
        return x * torch.tanh(torch.exp(x))

# ===== è‡ªå®šä¹‰ autogradï¼ˆå¯é€‰ï¼‰ï¼šåœ¨ç¨³å®šè¿‘ä¼¼ä¸‹æä¾›è§£æåå‘ï¼Œé¿å…é‡å¤å›¾è®¡ç®— =====
class _TeLUFunc(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x: torch.Tensor, cutoff: float):
        # å‰å‘ï¼šä¸ telu(stable=True) ç­‰ä»·
        large_pos = x > cutoff
        xc = torch.clamp(x, max=cutoff)
        e = torch.exp(xc.to(torch.float32)).to(x.dtype)
        t = torch.tanh(e)
        t = torch.where(large_pos, torch.ones_like(x), t)  # tanh(exp(x))â‰ˆ1
        y = x * t
        # ä¿å­˜ç”¨äºåå‘çš„ä¸­é—´é‡ï¼š
        # è®ºæ–‡ä¸­çš„ä¸€é˜¶å¯¼ï¼štanh(e^x) + x * e^x * (1 - tanh^2(e^x))ã€‚:contentReference[oaicite:4]{index=4}
        ctx.save_for_backward(x, t, e, large_pos)
        ctx.cutoff = cutoff
        return y

    @staticmethod
    def backward(ctx, grad_out):
        x, t, e, large_pos = ctx.saved_tensors
        # å¯¹ large_posï¼štâ‰ˆ1, å¯¼æ•°â‰ˆ1ï¼ˆå› ä¸º yâ‰ˆxï¼‰
        # å…¶ä½™åŒºåŸŸä½¿ç”¨è§£æå¯¼æ•°ï¼št + x * e * (1 - t^2)
        one = torch.ones_like(x)
        sech2 = (one - t * t)  # 1 - tanh^2
        grad_local = torch.where(
            large_pos,
            one,                       # dy/dx â‰ˆ 1
            t + x * e * sech2          # è®ºæ–‡ç»™å‡ºçš„ç²¾ç¡®å¯¼æ•°
        )
        return grad_out * grad_local, None

# ===== nn.Module å°è£… =====
class TeLU(nn.Module):
    """
    PyTorch æ¨¡å—ç‰ˆçš„ TeLUã€‚
    - é»˜è®¤å¯ç”¨æ•°å€¼ç¨³å®šè·¯å¾„ï¼ˆå¹¶å¸¦æœ‰è‡ªå®šä¹‰åå‘ï¼‰
    - å¦‚æœä½ æ›´åå¥½å®Œå…¨ç”± autograd æ¨å¯¼ï¼Œå¯è®¾ use_custom_backward=False
    """
    def __init__(self, stable: bool = True, cutoff: float = 20.0, use_custom_backward: bool = True):
        super().__init__()
        self.stable = stable
        self.cutoff = float(cutoff)
        self.use_custom_backward = bool(use_custom_backward)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.stable:
            if self.use_custom_backward and x.requires_grad:
                return _TeLUFunc.apply(x, self.cutoff)
            else:
                return telu(x, stable=True, cutoff=self.cutoff)
        else:
            return telu(x, stable=False)

# ===== ä¾¿æ·æµ‹è¯•ä¸ç¤ºä¾‹ =====
if __name__ == "__main__":
    x = torch.linspace(-10, 10, 5, requires_grad=True)
    act = TeLU()  # é»˜è®¤ç¨³å®š + è‡ªå®šä¹‰åå‘
    y = act(x)
    y.sum().backward()
    print("x:", x.detach())
    print("y:", y.detach())
    print("dy/dx:", x.grad)

    # ä¸çº¯å‡½æ•°å¼è°ƒç”¨
    z = telu(x.detach(), stable=True)
    print("telu(x):", z)

    # é›†æˆåˆ°æ¨¡å‹
    m = nn.Sequential(
        nn.Linear(16, 32),
        TeLU(),          # ç›´æ¥æ›¿æ¢ ReLU/SiLU
        nn.Linear(32, 10)
    )
    dummy = torch.randn(4, 16)
    out = m(dummy)
    print(out.shape)

    # TorchScriptï¼ˆå½“ use_custom_backward=False æ—¶æ›´æ˜“è„šæœ¬åŒ–ï¼‰
    m_script = nn.Sequential(
        nn.Linear(16, 32),
        TeLU(stable=True, use_custom_backward=False),  # å»ºè®®è¿™æ ·ä»¥ä¾¿ torchscript
        nn.Linear(32, 10)
    )
    scripted = torch.jit.script(m_script)
    scripted_out = scripted(dummy)
    print("scripted ok:", scripted_out.shape)